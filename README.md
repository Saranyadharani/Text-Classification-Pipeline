Text Classification Pipeline (DistilBERT + AG News)

A complete, end-to-end text classification pipeline built with Hugging Face Transformers, PyTorch, and scikit-learn, designed to run in one Google Colab cell.

ğŸš€ Features

Uses AG News dataset (World, Sports, Business, Sci/Tech)

Fine-tunes DistilBERT

Automatic train/test split

Training, evaluation, and prediction included

Fallback to synthetic data if download fails

Achieves ~94% accuracy

ğŸ“Š Dataset

AG News (127,600 samples)

4 balanced classes:

World

Sports

Business

Sci/Tech

ğŸ§  Model

distilbert-base-uncased

Hugging Face Trainer API

Cross-entropy loss with accuracy metric

â–¶ï¸ How to Run

Open Google Colab

Paste the full script into one cell

Run the cell â€” everything is handled automatically

ğŸ§ª Output

Training & validation metrics per epoch

Final test accuracy and loss

Predictions with confidence scores

Top-k class probabilities for sample inputs

ğŸ“¦ Requirements

Python 3.8+

transformers

datasets

torch

scikit-learn

pandas

numpy

(Automatically available in Google Colab)

âœ… Example Results

Test Accuracy: ~93â€“95%

Fast training with GPU support

ğŸ“Œ Use Cases

News classification

NLP fine-tuning demos

Text classification projects

Learning Hugging Face Trainer workflow
